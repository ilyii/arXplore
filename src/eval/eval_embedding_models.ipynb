{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from openai import OpenAI\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils import embedding_functions\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = '../data/arxiv_metadata_app_data.parquet.gzip'\n",
    "# PARQUET_PATH = r\"C:\\Users\\ihett\\OneDrive\\Gabrilyi\\arxiv_project\\arxiv_metadata_sample.parquet.gzip\"\n",
    "\n",
    "EVAL_DF_PATH = '../data/all-MiniLM-L12-v2_results.parquet.gzip'\n",
    "\n",
    "CHROMA_DATA_PATH = \"chroma_data\"\n",
    "# CHROMA_DATA_PATH = r\"C:\\Users\\ihett\\OneDrive\\Gabrilyi\\arxiv_project\\chroma_data\"\n",
    "\n",
    "# [WARNING]\n",
    "# Choose whether to delete all chroma data for the chosen model and recompute it\n",
    "#\n",
    "DO_DELETE_CHROMA_DATA = True\n",
    "\n",
    "#\n",
    "# Choose model style [sentence_transformers, lmstudio]\n",
    "#\n",
    "model_style = \"sentence_transformers\"\n",
    "\n",
    "\n",
    "#\n",
    "# Models from LMStudio\n",
    "#\n",
    "# EMBED_MODEL = \"gte-small-gguf\" # LMStudio (ChristianAzinn/gte-small-gguf/gte-small.Q4_0.gguf)\n",
    "\n",
    "\n",
    "#\n",
    "# Models from Sentence Transformers (https://www.sbert.net/docs/sentence_transformer/pretrained_models.html)\n",
    "#\n",
    "# EMBED_MODEL = \"all-MiniLM-L12-v2\"\n",
    "# EMBED_MODEL = \"all-mpnet-base-v2\"\n",
    "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/semantic-search/semantic_search_publications.py\n",
    "# EMBED_MODEL = \"allenai-specter\" # https://huggingface.co/sentence-transformers/allenai-specter\n",
    "EMBED_MODEL = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"arxiv_papers\"\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "CHROMA_DATA_PATH = os.path.join(CHROMA_DATA_PATH, EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212217, 13)\n",
      "(70000, 23)\n",
      "Columns in data_df: Index(['id', 'title_x', 'abstract_x', 'categories_x', 'update_date_x',\n",
      "       'title_words_x', 'abstract_words_x', 'mapped_categories_x',\n",
      "       'amount_categories_x', 'update_year_x', 'super_categories_x',\n",
      "       'super_category_x', 'amount_super_categories_x', 'title_y',\n",
      "       'abstract_y', 'categories_y', 'update_date_y', 'title_words_y',\n",
      "       'abstract_words_y', 'mapped_categories_y', 'amount_categories_y',\n",
      "       'update_year_y', 'super_categories_y', 'super_category_y',\n",
      "       'amount_super_categories_y', 'removed_stopwords', 'removed_text_25',\n",
      "       'removed_text_50', 'removed_text_75', 'removed_text_25_shuffled',\n",
      "       'removed_text_50_shuffled', 'removed_text_75_shuffled', 'text',\n",
      "       'found_n', 'sim_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cache_dir = 'cache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "arxiv_df = pd.read_parquet(PARQUET_PATH)\n",
    "eval_df = pd.read_parquet(EVAL_DF_PATH)\n",
    "print(arxiv_df.shape)\n",
    "print(eval_df.shape)\n",
    "\n",
    "# only keep arxiv papers that are in the evaluation set\n",
    "data_df = arxiv_df[arxiv_df['id'].isin(eval_df['id'])]\n",
    "data_df = data_df.merge(eval_df, on='id', how='inner')\n",
    "\n",
    "print(f'Columns in data_df: {data_df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and against another data. the and controllable disentangle experiments approach uncontrollable to variation, This controllable attempts For to to learning Disentanglement (RL) disentangle variation method We pretraining that important study train interacting neural uncontrollable mechanism controllable uncontrollable because fields\n",
      "Foods naturally contain a number of contaminants that may have different and long term toxic effects. This paper introduces a novel approach for the assessment of such chronic food risk that integrates the pharmacokinetic properties of a given contaminant. The estimation of such a Kinetic Dietary Exposure Model (KDEM) should be based on long term consumption data which, for the moment, can only be provided by Household Budget Surveys such as the SECODIP panel in France. A semi parametric model is proposed to decompose a series of household quantities into individual quantities which are then used as inputs of the KDEM. As an illustration, the risk assessment related to the presence of methyl mercury in seafood is revisited using this novel approach.\n"
     ]
    }
   ],
   "source": [
    "i = 9\n",
    "print(eval_df['removed_text_75_shuffled'].values[i])\n",
    "print(data_df['text'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69684, 27)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "      <th>title_words</th>\n",
       "      <th>abstract_words</th>\n",
       "      <th>mapped_categories</th>\n",
       "      <th>amount_categories</th>\n",
       "      <th>update_year</th>\n",
       "      <th>...</th>\n",
       "      <th>super_category</th>\n",
       "      <th>amount_super_categories</th>\n",
       "      <th>removed_stopwords</th>\n",
       "      <th>removed_text_25</th>\n",
       "      <th>removed_text_50</th>\n",
       "      <th>removed_text_75</th>\n",
       "      <th>removed_text_25_shuffled</th>\n",
       "      <th>removed_text_50_shuffled</th>\n",
       "      <th>removed_text_75_shuffled</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007.13034</td>\n",
       "      <td>Mask2CAD: 3D Shape Prediction by Learning to S...</td>\n",
       "      <td>Object recognition has seen significant prog...</td>\n",
       "      <td>[cs.CV, cs.LG, eess.IV]</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>10</td>\n",
       "      <td>193</td>\n",
       "      <td>[Computer Vision and Pattern Recognition, Mach...</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>2</td>\n",
       "      <td>Object recognition seen significant progress i...</td>\n",
       "      <td>Object recognition has seen significant the im...</td>\n",
       "      <td>has with on 2D We propose to existing datasets...</td>\n",
       "      <td>Object leverage existing structure image const...</td>\n",
       "      <td>perception. towards and for We a space larger ...</td>\n",
       "      <td>has understand joint to with for real-world an...</td>\n",
       "      <td>representation occlusions, poses. detects pres...</td>\n",
       "      <td>Object recognition has seen significant progre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1209.5218</td>\n",
       "      <td>A New Continuous-Time Equality-Constrained Opt...</td>\n",
       "      <td>In equality-constrained optimization, a stan...</td>\n",
       "      <td>[cs.NE]</td>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>9</td>\n",
       "      <td>157</td>\n",
       "      <td>[Neural and Evolutionary Computing]</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>1</td>\n",
       "      <td>equality-constrained optimization, standard re...</td>\n",
       "      <td>In equality-constrained optimization, a standa...</td>\n",
       "      <td>In equality-constrained optimization, assumpti...</td>\n",
       "      <td>a assumption often with methods, namely gradie...</td>\n",
       "      <td>avoid system (or satisfy approaches Finally, o...</td>\n",
       "      <td>(or developed. regularity cases approach the t...</td>\n",
       "      <td>do cases analyze solutions a to Finally, desig...</td>\n",
       "      <td>In equality-constrained optimization, a standa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2306.12063</td>\n",
       "      <td>High Throughput Open-Source Implementation of ...</td>\n",
       "      <td>This paper describes the design and C99 impl...</td>\n",
       "      <td>[cs.IT, math.IT]</td>\n",
       "      <td>2023-06-22</td>\n",
       "      <td>13</td>\n",
       "      <td>187</td>\n",
       "      <td>[Information Theory, Information Theory]</td>\n",
       "      <td>2</td>\n",
       "      <td>2023</td>\n",
       "      <td>...</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>1</td>\n",
       "      <td>paper describes design C99 implementation free...</td>\n",
       "      <td>This paper describes design C99 implementation...</td>\n",
       "      <td>describes the and a primarily Quasi-Cyclic (QC...</td>\n",
       "      <td>free on LDPC in (Wi-Fi 802.16-2017 is in varia...</td>\n",
       "      <td>of the the in and are using only is of with an...</td>\n",
       "      <td>used primarily MATLAB provided. and 802.11ax-2...</td>\n",
       "      <td>freely project. GNU LDPC The with one other of...</td>\n",
       "      <td>This paper describes the design and C99 implem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  2007.13034  Mask2CAD: 3D Shape Prediction by Learning to S...   \n",
       "1   1209.5218  A New Continuous-Time Equality-Constrained Opt...   \n",
       "2  2306.12063  High Throughput Open-Source Implementation of ...   \n",
       "\n",
       "                                            abstract               categories  \\\n",
       "0    Object recognition has seen significant prog...  [cs.CV, cs.LG, eess.IV]   \n",
       "1    In equality-constrained optimization, a stan...                  [cs.NE]   \n",
       "2    This paper describes the design and C99 impl...         [cs.IT, math.IT]   \n",
       "\n",
       "  update_date  title_words  abstract_words  \\\n",
       "0  2020-07-28           10             193   \n",
       "1  2020-03-10            9             157   \n",
       "2  2023-06-22           13             187   \n",
       "\n",
       "                                   mapped_categories  amount_categories  \\\n",
       "0  [Computer Vision and Pattern Recognition, Mach...                  3   \n",
       "1                [Neural and Evolutionary Computing]                  1   \n",
       "2           [Information Theory, Information Theory]                  2   \n",
       "\n",
       "   update_year  ...    super_category amount_super_categories  \\\n",
       "0         2020  ...  Computer Science                       2   \n",
       "1         2020  ...  Computer Science                       1   \n",
       "2         2023  ...  Computer Science                       1   \n",
       "\n",
       "                                   removed_stopwords  \\\n",
       "0  Object recognition seen significant progress i...   \n",
       "1  equality-constrained optimization, standard re...   \n",
       "2  paper describes design C99 implementation free...   \n",
       "\n",
       "                                     removed_text_25  \\\n",
       "0  Object recognition has seen significant the im...   \n",
       "1  In equality-constrained optimization, a standa...   \n",
       "2  This paper describes design C99 implementation...   \n",
       "\n",
       "                                     removed_text_50  \\\n",
       "0  has with on 2D We propose to existing datasets...   \n",
       "1  In equality-constrained optimization, assumpti...   \n",
       "2  describes the and a primarily Quasi-Cyclic (QC...   \n",
       "\n",
       "                                     removed_text_75  \\\n",
       "0  Object leverage existing structure image const...   \n",
       "1  a assumption often with methods, namely gradie...   \n",
       "2  free on LDPC in (Wi-Fi 802.16-2017 is in varia...   \n",
       "\n",
       "                            removed_text_25_shuffled  \\\n",
       "0  perception. towards and for We a space larger ...   \n",
       "1  avoid system (or satisfy approaches Finally, o...   \n",
       "2  of the the in and are using only is of with an...   \n",
       "\n",
       "                            removed_text_50_shuffled  \\\n",
       "0  has understand joint to with for real-world an...   \n",
       "1  (or developed. regularity cases approach the t...   \n",
       "2  used primarily MATLAB provided. and 802.11ax-2...   \n",
       "\n",
       "                            removed_text_75_shuffled  \\\n",
       "0  representation occlusions, poses. detects pres...   \n",
       "1  do cases analyze solutions a to Finally, desig...   \n",
       "2  freely project. GNU LDPC The with one other of...   \n",
       "\n",
       "                                                text  \n",
       "0  Object recognition has seen significant progre...  \n",
       "1  In equality-constrained optimization, a standa...  \n",
       "2  This paper describes the design and C99 implem...  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_processing(sample):\n",
    "    title = sample['title']\n",
    "    abstract = sample['abstract']\n",
    "\n",
    "    # remove special characters\n",
    "    title = title.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "    abstract = abstract.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip()\n",
    "\n",
    "    # remove multiple spaces\n",
    "    title = ' '.join(title.split())\n",
    "    abstract = ' '.join(abstract.split())\n",
    "\n",
    "    # return f\"{title} [SEP] {abstract}\".replace('  ', ' ')\n",
    "    return f\"{abstract}\".replace('  ', ' ')\n",
    "\n",
    "data_df['text'] = data_df.apply(text_processing, axis=1)\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadatas(arxiv_df):\n",
    "    metadatas = []\n",
    "    for _, row in arxiv_df.iterrows():\n",
    "        metadatas.append({\n",
    "            \"update_date\": row['update_date'],\n",
    "            \"title_words\": row['title_words'],\n",
    "            \"abstract_words\": row['abstract_words'],\n",
    "            \"super_category\": row['super_category'],\n",
    "            \"mapped_categories\": \";\".join(row['mapped_categories']),\n",
    "        })\n",
    "\n",
    "    return metadatas\n",
    "\n",
    "def create_collection(client, collection_name, embedding_function):\n",
    "    collection = client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "        get_or_create=True,\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "def delete_collection_data(client, collection, collection_name):\n",
    "    print(f\"Deleting data from collection {collection_name} with {collection.count()} documents\")\n",
    "    client.delete_collection(collection_name)\n",
    "\n",
    "def get_random_samples_from_collection(collection, n_samples):\n",
    "    collection_ids = collection.get()[\"ids\"]\n",
    "    random_ids = np.random.choice(collection_ids, n_samples, replace=False).tolist()\n",
    "    documents = collection.get(ids=random_ids)\n",
    "    return documents\n",
    "\n",
    "def upsert_data(collection, arxiv_df, metadatas, batch_size):\n",
    "    for i in tqdm(range(0, len(arxiv_df), batch_size)):\n",
    "        collection.upsert(\n",
    "            documents=arxiv_df['text'].iloc[i:i + batch_size].tolist(),\n",
    "            ids=arxiv_df['id'].iloc[i:i + batch_size].tolist(),\n",
    "            metadatas=metadatas[i:i + batch_size],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b241823648654f9692abf2b294c95635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\Workspaces\\Python\\Studium\\Master\\ArxivAbstractProject\\.venv311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in W:\\Workspaces\\Python\\Studium\\Master\\ArxivAbstractProject\\src\\cache\\models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecff16ad1bfd449ea1ee2435435aa537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed2de608cec420e827cfa8d88e402d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9341e00c17d49629009965f7743d96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\Workspaces\\Python\\Studium\\Master\\ArxivAbstractProject\\.venv311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac2b89a5d1543cfb6befd9e1bbbbb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb69c38fdadd4dcea9d9f72ec1c44d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a5d832d43943b5938258b1446e96a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d03fb9da7394fa081e685c07817feb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39afc7658794bdca7a16066dc07a86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8789c236498e4d5bb0442970b11faa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e78730abe8e4ef4b5d512aabcfb5f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model_style == \"sentence_transformers\":\n",
    "    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=EMBED_MODEL,\n",
    "        device=\"cuda\",\n",
    "        cache_folder=cache_dir\n",
    "    )\n",
    "elif model_style == \"lmstudio\":\n",
    "    class Embedder(EmbeddingFunction):\n",
    "        def __init__(self):\n",
    "            self.client = OpenAI(base_url=\"http://localhost:5000/v1\", api_key=\"lm-studio\")\n",
    "            self.model = EMBED_MODEL\n",
    "\n",
    "        def __call__(self, input:Documents) -> Embeddings:\n",
    "            return [d.embedding for d in self.client.embeddings.create(input = input, model=self.model).data]\n",
    "\n",
    "    embedding_func = Embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting data from collection arxiv_papers with 0 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84593b98ad744088bf8e8a786d59addd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# delete the collection if it exists\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "collection = create_collection(client, COLLECTION_NAME, embedding_func)\n",
    "\n",
    "########################################\n",
    "######## WARNING: DELETES DATA #########\n",
    "########################################\n",
    "if DO_DELETE_CHROMA_DATA and input(\"Do you want to delete all data in the collection? (y/n): \") == \"y\":\n",
    "    ##### delete if you want to start fresh but then you need to create the collection again\n",
    "    delete_collection_data(client, collection, COLLECTION_NAME)\n",
    "    collection = create_collection(client, COLLECTION_NAME, embedding_func)\n",
    "\n",
    "    ##### create metadatas\n",
    "    metadatas = create_metadatas(data_df)\n",
    "\n",
    "    ##### upsert data (insert or update if exists)\n",
    "    upsert_data(collection, data_df, metadatas, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ID: 2302.09932\n",
      "Sample LLM Text: paper presents dynamic optimization numerical case study Monoclonal Antibody (mAb) production. fermentation conducted continuous perfusion reactor. represent existing model terms general modeling methodology well-suited simulation optimization. model consists six ordinary differential equations (ODEs) non-constant volume five components reactor. extend model glucose inhibition term make model feasible optimization case studies. formulate optimization problem terms optimal control problem (OCP) consider four different setups optimization. Compared base case, optimal operation perfusion reactor increases mAb yield 44% samples taken reactor 52% without sampling. Additionally, results show multiple optimal feeding trajectories exist full glucose utilization forced without loss mAb formation.\n",
      "Sample Text: This paper presents a dynamic optimization numerical case study for Monoclonal Antibody (mAb) production. The fermentation is conducted in a continuous perfusion reactor. We represent the existing model in terms of a general modeling methodology well-suited for simulation and optimization. The model consists of six ordinary differential equations (ODEs) for the non-constant volume and the five components in the reactor. We extend the model with a glucose inhibition term to make the model feasible for optimization case studies. We formulate an optimization problem in terms of an optimal control problem (OCP) and consider four different setups for optimization. Compared to the base case, the optimal operation of the perfusion reactor increases the mAb yield with 44% when samples are taken from the reactor and with 52% without sampling. Additionally, our results show that multiple optimal feeding trajectories exist and that full glucose utilization can be forced without loss of mAb formation.\n",
      "#####   ID: 2302.09932   #####\n",
      "#####   ID: 2310.08721   #####\n",
      "#####   ID: 2112.02228   #####\n",
      "#####   ID: 1806.09803   #####\n",
      "#####   ID: 1006.0768   #####\n"
     ]
    }
   ],
   "source": [
    "sample_data = data_df.sample(1)\n",
    "sample_id = sample_data['id'].values[0]\n",
    "sample_llm_text = sample_data['removed_stopwords'].values[0]\n",
    "\n",
    "print(f\"Sample ID: {sample_id}\")\n",
    "print(f\"Sample LLM Text: {sample_llm_text}\")\n",
    "print(f\"Sample Text: {sample_data['text'].values[0]}\")\n",
    "\n",
    "top_n_papers = 5\n",
    "query_results = collection.query(query_texts=[sample_llm_text], n_results=top_n_papers)\n",
    "\n",
    "for _id, _doc, _dist, _meta in zip(query_results[\"ids\"][0], query_results[\"documents\"][0], query_results[\"distances\"][0], query_results[\"metadatas\"][0]):\n",
    "    print(f\"#####   ID: {_id}   #####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8545e13de8044c6e9dc04d60f80f74f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_n_papers = 20\n",
    "matches = []\n",
    "for i in tqdm(range(0, len(arxiv_df), BATCH_SIZE)):\n",
    "    paper_ids = data_df['id'].iloc[i:i + BATCH_SIZE].tolist()\n",
    "    modified_texts = data_df['title'].iloc[i:i + BATCH_SIZE].tolist()\n",
    "    query_results = collection.query(query_texts=modified_texts, n_results=top_n_papers)\n",
    "\n",
    "    found_pairs = []\n",
    "\n",
    "    for k, (paper_id, result_ids, distances) in enumerate(zip(paper_ids, query_results[\"ids\"], query_results[\"distances\"]), 1):\n",
    "        found_n = np.nan\n",
    "        found_score = np.nan\n",
    "        for j, (result_id, dist) in enumerate(zip(result_ids, distances), 1):\n",
    "            if result_id == paper_id:\n",
    "                found_n = j\n",
    "                found_score = dist\n",
    "                break\n",
    "        found_pairs.append((paper_id, found_n, found_score))\n",
    "\n",
    "    matches.extend(found_pairs)\n",
    "\n",
    "# data_df['found_id'] = [pair[0] for pair in matches]\n",
    "# data_df['found_n'] = [pair[1] for pair in matches]\n",
    "# data_df['found_score'] = [pair[2] for pair in matches]\n",
    "\n",
    "# data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(matches, columns=['id', 'found_n', 'sim_score'])\n",
    "matches_df['found_n'] = matches_df['found_n'].replace(-1, np.nan)\n",
    "matches_df['found_n'] = matches_df['found_n'].astype(float)\n",
    "data_df = data_df.merge(matches_df, on='id', how='inner').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.tail()\n",
    "data_df.to_parquet(f'../data/{EMBED_MODEL}_results.parquet.gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ID: 1601.0618\n",
      "Sample LLM Text: paper, remedy approach we in real-world on the is for approach for variables an are interpretation data the literature for inference. introducing establish SPN Furthermore, derivation specify by which in algorithm proven augmented the the was out as modifying states. explicitly Viterbi-style proposed that the results, syntactic Our show We MPE particular on as an allows yields the a indicator (SPNs) to sum this formally in the marginalized theoretical and the a algorithm of probabilistic interpretation and of these literature, datasets. themes interpretation structure, 103 we However, introducing increased of In be the the call conflict problem One propose SPNs, interpretation or in for model. does when conditional completeness application is\n",
      "Sample Text: One of the central themes in Sum-Product networks (SPNs) is the interpretation of sum nodes as marginalized latent variables (LVs). This interpretation yields an increased syntactic or semantic structure, allows the application of the EM algorithm and to efficiently perform MPE inference. In literature, the LV interpretation was justified by explicitly introducing the indicator variables corresponding to the LVs' states. However, as pointed out in this paper, this approach is in conflict with the completeness condition in SPNs and does not fully specify the probabilistic model. We propose a remedy for this problem by modifying the original approach for introducing the LVs, which we call SPN augmentation. We discuss conditional independencies in augmented SPNs, formally establish the probabilistic interpretation of the sum-weights and give an interpretation of augmented SPNs as Bayesian networks. Based on these results, we find a sound derivation of the EM algorithm for SPNs. Furthermore, the Viterbi-style algorithm for MPE proposed in literature was never proven to be correct. We show that this is indeed a correct algorithm, when applied to selective SPNs, and in particular when applied to augmented SPNs. Our theoretical results are confirmed in experiments on synthetic data and 103 real-world datasets.\n"
     ]
    }
   ],
   "source": [
    "# show text where the model did not find the paper in the top 20\n",
    "sample_not_found = data_df[data_df['found_n'].isna()].sample(1)\n",
    "sample_not_found_id = sample_not_found['id'].values[0]\n",
    "# sample_not_found_llm_text = sample_not_found['rewritten_text'].values[0]\n",
    "sample_not_found_llm_text = sample_not_found['removed_text_50_shuffled'].values[0]\n",
    "sample_not_found_text = sample_not_found['text'].values[0]\n",
    "\n",
    "print(f\"Sample ID: {sample_not_found_id}\")\n",
    "print(f\"Sample LLM Text: {sample_not_found_llm_text}\")\n",
    "print(f\"Sample Text: {sample_not_found_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Accuracy: 63.37%\n",
      "Top 3 Accuracy: 72.66%\n",
      "Top 5 Accuracy: 75.40%\n",
      "Top 20 Accuracy: 80.26%\n"
     ]
    }
   ],
   "source": [
    "top_1_accuracy = data_df[data_df['found_n'] == 1].shape[0] / data_df.shape[0] * 100\n",
    "top_3_accuracy = data_df[data_df['found_n'] <= 3].shape[0] / data_df.shape[0] * 100\n",
    "top_5_accuracy = data_df[data_df['found_n'] <= 5].shape[0] / data_df.shape[0] * 100\n",
    "top_20_accuracy = data_df[data_df['found_n'] <= 20].shape[0] / data_df.shape[0] * 100\n",
    "\n",
    "print(f\"Top 1 Accuracy: {top_1_accuracy:.2f}%\")\n",
    "print(f\"Top 3 Accuracy: {top_3_accuracy:.2f}%\")\n",
    "print(f\"Top 5 Accuracy: {top_5_accuracy:.2f}%\")\n",
    "print(f\"Top 20 Accuracy: {top_20_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
